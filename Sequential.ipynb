{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sequential.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmU1t08Mpn3W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47a95629-2976-4841-e797-0baf59f03c0d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mpAankWpsfP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22350cab-c881-4c31-b4bc-ce4bca360688"
      },
      "source": [
        "cd gdrive/My Drive/CS 598 GW/Project/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'gdrive/My Drive/CS 598 GW/Project/'\n",
            "/content/gdrive/My Drive/CS 598 GW/Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dulauA9XpvQZ"
      },
      "source": [
        "###########################################         DEPENDENCIES             #####################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byq1Zg_wpvN8"
      },
      "source": [
        "import shutil\n",
        "import os.path\n",
        "from os import path\n",
        "import pandas as pd\n",
        "#import matplotlib.pyplot as plt\n",
        "#import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "#from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.distributed as dist\n",
        "import torchvision\n",
        "\n",
        "import h5py\n",
        "import cv2\n",
        "\n",
        "from multiprocessing import Pool"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR-uwp9fpvL2"
      },
      "source": [
        "###########################################         HELPER FUNCTIONS BELOW             #####################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XYDixbApvJi"
      },
      "source": [
        "def data_splitter(MAIN_DIR, TRAIN_DATA):\n",
        "\n",
        "\t# list of training & test videos\n",
        "\ttotal_data = list(os.listdir(os.path.join(MAIN_DIR,TRAIN_DATA)))\n",
        "\ttrain_df = get_meta_from_json('data/train_sample_videos/metadata.json')\n",
        "\ttrain_df.reset_index(inplace=True)\n",
        "\tdf = pd.DataFrame(columns = ['files', 'labels'])\n",
        "\tdf.files = train_df['index']\n",
        "\n",
        "\tfor i in range(len(df)):\n",
        "\t\tif train_df.iloc[i,1] == 'FAKE':\n",
        "\t\t\tdf.iloc[i,1] = 1\n",
        "\t\telif train_df.iloc[i,1] == 'REAL':\n",
        "\t\t\tdf.iloc[i,1] = 0\n",
        "\n",
        "\tX = list(df.files)\n",
        "\ty = list(df.labels)\n",
        "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\treturn [X_train, y_train], [X_test, y_test] , df\n",
        "\n",
        "def get_meta_from_json(path):\n",
        "    df = pd.read_json(path)\n",
        "    df = df.T\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDshFxbPu5MI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgxwwavtpvHR"
      },
      "source": [
        "def loadSequence(args):\n",
        "    (filename,augment) = args\n",
        "    vid = []\n",
        "    cap = cv2.VideoCapture()\n",
        "    path = os.path.join(MAIN_DIR,TRAIN_DIR,filename)\n",
        "    cap.open(path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Failed to open input video\")\n",
        "\n",
        "    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "    frame_idx = 0\n",
        "\n",
        "    while frame_idx < frame_count:\n",
        "        ret, frame = cap.read()\n",
        "        vid.append(frame)\n",
        "\n",
        "\n",
        "        if not ret:\n",
        "            print (\"Failed to get the frame {}\".format(frameId))\n",
        "            continue\n",
        "        frame_idx += 10\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "    vid_arr = np.array(vid).reshape(len(vid), 1080, 1920, 3)\n",
        "    \n",
        "    mean = np.asarray([0.433, 0.4045, 0.3776],np.float32)\n",
        "    std = np.asarray([0.1519876, 0.14855877, 0.156976],np.float32)\n",
        "\n",
        "    curr_w = 1080\n",
        "    curr_h = 1920\n",
        "    height = width = 500\n",
        "    num_of_frames = 16\n",
        "\n",
        "    (filename,augment) = args\n",
        "\n",
        "    data = np.zeros((3,num_of_frames,height,width),dtype=np.float32)\n",
        "\n",
        "    try:\n",
        "        nFrames = len(vid)\n",
        "        frame_index = np.random.randint(nFrames - num_of_frames)\n",
        "        video = vid_arr[frame_index:(frame_index + num_of_frames)]\n",
        "\n",
        "        if(augment==True):\n",
        "            ## RANDOM CROP - crop 70-100% of original size\n",
        "            ## don't maintain aspect ratio\n",
        "            resize_factor_w = 0.3*np.random.rand()+0.7\n",
        "            resize_factor_h = 0.3*np.random.rand()+0.7\n",
        "            w1 = int(curr_w*resize_factor_w)\n",
        "            h1 = int(curr_h*resize_factor_h)\n",
        "            w = np.random.randint(curr_w-w1)\n",
        "            h = np.random.randint(curr_h-h1)\n",
        "            random_crop = np.random.randint(2)\n",
        "\n",
        "            ## Random Flip\n",
        "            random_flip = np.random.randint(2)\n",
        "\n",
        "            ## Brightness +/- 15\n",
        "            brightness = 30\n",
        "            random_add = np.random.randint(brightness+1) - brightness/2.0\n",
        "\n",
        "            data = []\n",
        "            for frame in video:\n",
        "                if(random_crop):\n",
        "                    frame = frame[h:(h+h1),w:(w+w1),:]\n",
        "                if(random_flip):\n",
        "                    frame = cv2.flip(frame,1)\n",
        "                frame = cv2.resize(frame,(width,height))\n",
        "                frame = frame.astype(np.float32)\n",
        "                \n",
        "                frame += random_add\n",
        "                frame[frame>255] = 255.0\n",
        "                frame[frame<0] = 0.0\n",
        "\n",
        "                frame = frame/255.0\n",
        "                frame = (frame - mean)/std\n",
        "                data.append(frame)\n",
        "            data = np.asarray(data)\n",
        "\n",
        "        else:\n",
        "            # don't augment\n",
        "            data = []\n",
        "            for frame in video:\n",
        "                frame = cv2.resize(frame,(width,height))\n",
        "                frame = frame.astype(np.float32)\n",
        "                frame = frame/255.0\n",
        "                frame = (frame - mean)/std\n",
        "                data.append(frame)\n",
        "            data = np.asarray(data)\n",
        "\n",
        "        data = data.transpose(3,0,1,2)\n",
        "    except:\n",
        "        print(\"Exception: \" + filename)\n",
        "        data = np.array([])\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RgTrdPPpvE-"
      },
      "source": [
        "###########################################         HELPER FUNCTIONS ABOVE             #####################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KENA67RZpvCm"
      },
      "source": [
        "# DATASET\n",
        "\n",
        "MAIN_DIR = 'data'\n",
        "TRAIN_DIR = 'train_sample_videos'\n",
        "train, test, df = data_splitter(MAIN_DIR,TRAIN_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzlTJn7Tq-Dv"
      },
      "source": [
        "IMAGE_SIZE = 500\n",
        "NUM_CLASSES = 2\n",
        "# batch_size = 32\n",
        "batch_size = 10\n",
        "lr = 0.0001\n",
        "num_of_epochs = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77mPmogZrLAP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "485ab2ad-d92f-446d-8824-7bc7aafb5397"
      },
      "source": [
        "import resnet_3d\n",
        "dir = 'data/'\n",
        "model =  resnet_3d.resnet50(sample_size=IMAGE_SIZE, sample_duration=16)\n",
        "pretrained = torch.load(dir + 'resnet-50-kinetics.pth')\n",
        "keys = [k for k,v in pretrained['state_dict'].items()]\n",
        "pretrained_state_dict = {k[7:]: v.cpu() for k, v in pretrained['state_dict'].items()}\n",
        "model.load_state_dict(pretrained_state_dict)\n",
        "model.fc = nn.Linear(model.fc.weight.shape[1],NUM_CLASSES)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad_(False)\n",
        "\n",
        "# for param in model.conv1.parameters():\n",
        "#     param.requires_grad_(True)\n",
        "# for param in model.bn1.parameters():\n",
        "#     param.requires_grad_(True)\n",
        "# for param in model.layer1.parameters():\n",
        "#     param.requires_grad_(True)\n",
        "# for param in model.layer2.parameters():\n",
        "#     param.requires_grad_(True)\n",
        "# for param in model.layer3.parameters():\n",
        "#     param.requires_grad_(True)\n",
        "for param in model.layer4[0].parameters():\n",
        "    param.requires_grad_(True)\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad_(True)\n",
        "\n",
        "params = []\n",
        "# for param in model.conv1.parameters():\n",
        "#     params.append(param)\n",
        "# for param in model.bn1.parameters():\n",
        "#     params.append(param)\n",
        "# for param in model.layer1.parameters():\n",
        "#     params.append(param)\n",
        "# for param in model.layer2.parameters():\n",
        "#     params.append(param)\n",
        "# for param in model.layer3.parameters():\n",
        "#     params.append(param)\n",
        "for param in model.layer4[0].parameters():\n",
        "    params.append(param)\n",
        "for param in model.fc.parameters():\n",
        "    params.append(param)\n",
        "\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "optimizer = optim.Adam(params,lr=lr)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "pool_threads = Pool(8,maxtasksperchild=200)\n",
        "\n",
        "for epoch in range(0,num_of_epochs):\n",
        "\n",
        "    ###### TRAIN\n",
        "    train_accu = []\n",
        "    model.train()\n",
        "    random_indices = np.random.permutation(len(train[0]))\n",
        "    start_time = time.time()\n",
        "    for i in range(0, len(train[0])-batch_size,batch_size):\n",
        "\n",
        "        augment = True\n",
        "        video_list = [(train[0][k],augment)\n",
        "                       for k in random_indices[i:(batch_size+i)]]\n",
        "        data = pool_threads.map(loadSequence,video_list)\n",
        "\n",
        "        next_batch = 0\n",
        "        for video in data:\n",
        "            if video.size==0: # there was an exception, skip this\n",
        "                next_batch = 1\n",
        "        if(next_batch==1):\n",
        "            continue\n",
        "\n",
        "        x = np.asarray(data,dtype=np.float32)\n",
        "        x = Variable(torch.FloatTensor(x),requires_grad=False).cuda().contiguous()\n",
        "        y = np.array(train[1])\n",
        "        y = y[random_indices[i:(batch_size+i)]]\n",
        "        y = torch.from_numpy(y).cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            h = model.conv1(x)\n",
        "            h = model.bn1(h)\n",
        "            h = model.relu(h)\n",
        "            h = model.maxpool(h)\n",
        "\n",
        "            h = model.layer1(h)\n",
        "            h = model.layer2(h)\n",
        "            h = model.layer3(h)\n",
        "        h = model.layer4[0](h)\n",
        "\n",
        "        h = model.avgpool(h)\n",
        "\n",
        "        h = h.view(h.size(0), -1)\n",
        "        output = model.fc(h)\n",
        "\n",
        "        # output = model(x)\n",
        "\n",
        "        loss = criterion(output, y)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        prediction = output.data.max(1)[1]\n",
        "        accuracy = ( float( prediction.eq(y.data).sum() ) /float(batch_size))*100.0\n",
        "        if(epoch==0):\n",
        "            print(i,accuracy)\n",
        "        train_accu.append(accuracy)\n",
        "    accuracy_epoch = np.mean(train_accu)\n",
        "    print(epoch, accuracy_epoch,time.time()-start_time)\n",
        "\n",
        "    ##### TEST\n",
        "    model.eval()\n",
        "    test_accu = []\n",
        "    random_indices = np.random.permutation(len(test[0]))\n",
        "    t1 = time.time()\n",
        "    for i in range(0,len(test[0])-batch_size,batch_size):\n",
        "        augment = False\n",
        "        video_list = [(test[0][k],augment) \n",
        "                        for k in random_indices[i:(batch_size+i)]]\n",
        "        data = pool_threads.map(loadSequence,video_list)\n",
        "\n",
        "        next_batch = 0\n",
        "        for video in data:\n",
        "            if video.size==0: # there was an exception, skip this batch\n",
        "                next_batch = 1\n",
        "        if(next_batch==1):\n",
        "            continue\n",
        "\n",
        "        x = np.asarray(data,dtype=np.float32)\n",
        "        x = Variable(torch.FloatTensor(x)).cuda().contiguous()\n",
        "        y = np.array(test[1])\n",
        "\n",
        "        y = y[random_indices[i:(batch_size+i)]]\n",
        "        y = torch.from_numpy(y).cuda()\n",
        "\n",
        "        # with torch.no_grad():\n",
        "        #     output = model(x)\n",
        "        with torch.no_grad():\n",
        "            h = model.conv1(x)\n",
        "            h = model.bn1(h)\n",
        "            h = model.relu(h)\n",
        "            h = model.maxpool(h)\n",
        "\n",
        "            h = model.layer1(h)\n",
        "            h = model.layer2(h)\n",
        "            h = model.layer3(h)\n",
        "            h = model.layer4[0](h)\n",
        "            # h = model.layer4[1](h)\n",
        "\n",
        "            h = model.avgpool(h)\n",
        "\n",
        "            h = h.view(h.size(0), -1)\n",
        "            output = model.fc(h)\n",
        "\n",
        "        prediction = output.data.max(1)[1]\n",
        "        accuracy = ( float( prediction.eq(y.data).sum() ) /float(batch_size))*100.0\n",
        "        test_accu.append(accuracy)\n",
        "        accuracy_test = np.mean(test_accu)\n",
        "\n",
        "    print('Testing',accuracy_test,time.time()-t1)\n",
        "\n",
        "torch.save(model,'3d_resnet.model')\n",
        "pool_threads.close()\n",
        "pool_threads.terminate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/CS 598 GW/Project/resnet_3d.py:145: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
            "  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 70.0\n",
            "10 70.0\n",
            "20 70.0\n",
            "30 40.0\n",
            "40 70.0\n",
            "Failed to open input video\n",
            "Exception: bvgwelbeof.mp4\n",
            "60 70.0\n",
            "70 70.0\n",
            "80 80.0\n",
            "90 60.0\n",
            "100 40.0\n",
            "110 70.0\n",
            "120 80.0\n",
            "130 100.0\n",
            "140 70.0\n",
            "150 70.0\n",
            "160 90.0\n",
            "170 70.0\n",
            "180 100.0\n",
            "190 60.0\n",
            "200 70.0\n",
            "210 90.0\n",
            "220 60.0\n",
            "230 90.0\n",
            "240 100.0\n",
            "250 70.0\n",
            "0 73.2 6963.494796514511\n",
            "Testing 85.38461538461539 3534.4263665676117\n",
            "Failed to open input video\n",
            "Exception: bvgwelbeof.mp4\n",
            "1 79.2 7070.585575580597\n",
            "Testing 84.61538461538461 3464.7863833904266\n",
            "Failed to open input video\n",
            "Exception: bvgwelbeof.mp4\n",
            "2 77.6 6712.629623889923\n",
            "Testing 84.61538461538461 3350.4906520843506\n",
            "Failed to open input video\n",
            "Exception: bvgwelbeof.mp4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDGBtaYEuijB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f543zsgvpvAH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ykOE9uNrK84"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX1bQZEZrK6Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWwi1QhSrK0j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}